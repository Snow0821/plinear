##### Found out that square models way better than non-square models.

###### Hypothesis

While backpropagation if the tensor got smaller, the change should spread thinner.

Need compensation since unlike traditional linear, quantized models does not show the change until it passes threshold.

## visualization (Not finised for documentation)

## Exhaustive search of 3 x 3 CNN (Only Idea)

Since I parrallelized layers, each 3 x 3 CNN layer can be brute forced in 2^9 \* 2 weights for ternary, which is very cheap against previous models.

Even the model is same, the layer is still at least 9 times smaller even if the model seeked through every cases.

And we can reduce the model with simple searching tasks.

I believe this can be used to vectorize images in proper size of vector which can be reused for image generation or more.

I guess vectorizing concepts and dynamically allocating them with layers would be the final goal of this project.